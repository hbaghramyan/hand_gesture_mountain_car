{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841bf530",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "   ### Description\n",
    "   The Mountain Car MDP is a deterministic MDP that consists of a car placed stochastically\n",
    "   at the bottom of a sinusoidal valley, with the only possible actions being the accelerations\n",
    "   that can be applied to the car in either direction. The goal of the MDP is to strategically\n",
    "   accelerate the car to reach the goal state on top of the right hill. There are two versions\n",
    "   of the mountain car domain in gym: one with discrete actions and one with continuous.\n",
    "   This version is the one with discrete actions.\n",
    "   This MDP first appeared in [Andrew Moore's PhD Thesis (1990)](https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-209.pdf)\n",
    "   ```\n",
    "    @TECHREPORT{Moore90efficientmemory-based,\n",
    "        author = {Andrew William Moore},\n",
    "        title = {Efficient Memory-based Learning for Robot Control},\n",
    "        institution = {University of Cambridge},\n",
    "        year = {1990}\n",
    "    }\n",
    "   ```\n",
    "   ### Observation Space\n",
    "   The observation is a `ndarray` with shape `(2,)` where the elements correspond to the following:\n",
    "   | Num | Observation                          | Min  | Max | Unit         |\n",
    "   |-----|--------------------------------------|------|-----|--------------|\n",
    "   | 0   | position of the car along the x-axis | -Inf | Inf | position (m) |\n",
    "   | 1   | velocity of the car                  | -Inf | Inf | position (m) |\n",
    "   ### Action Space\n",
    "   There are 3 discrete deterministic actions:\n",
    "   | Num | Observation             | Value | Unit         |\n",
    "   |-----|-------------------------|-------|--------------|\n",
    "   | 0   | Accelerate to the left  | Inf   | position (m) |\n",
    "   | 1   | Don't accelerate        | Inf   | position (m) |\n",
    "   | 2   | Accelerate to the right | Inf   | position (m) |\n",
    "   ### Transition Dynamics:\n",
    "   Given an action, the mountain car follows the following transition dynamics:\n",
    "   *velocity<sub>t+1</sub> = velocity<sub>t</sub> + (action - 1) * force - cos(3 * position<sub>t</sub>) * gravity*\n",
    "   *position<sub>t+1</sub> = position<sub>t</sub> + velocity<sub>t+1</sub>*\n",
    "   where force = 0.001 and gravity = 0.0025. The collisions at either end are inelastic with the velocity set to 0\n",
    "   upon collision with the wall. The position is clipped to the range `[-1.2, 0.6]` and\n",
    "   velocity is clipped to the range `[-0.07, 0.07]`.\n",
    "   ### Reward:\n",
    "   The goal is to reach the flag placed on top of the right hill as quickly as possible, as such the agent is\n",
    "   penalised with a reward of -1 for each timestep.\n",
    "   ### Starting State\n",
    "   The position of the car is assigned a uniform random value in *[-0.6 , -0.4]*.\n",
    "   The starting velocity of the car is always assigned to 0.\n",
    "   ### Episode End\n",
    "   The episode ends if either of the following happens:\n",
    "   1. Termination: The position of the car is greater than or equal to 0.5 (the goal position on top of the right hill)\n",
    "   2. Truncation: The length of the episode is 200.\n",
    "   ### Arguments\n",
    "   ```\n",
    "    gym.make('MountainCar-v0')\n",
    "   ```\n",
    "   ### Version History\n",
    "   * v0: Initial versions release (1.0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061a1af1-5d49-4379-a2c8-7b354ef0f5ad",
   "metadata": {},
   "source": [
    "# ¿Qué es Gym de OpenAI?\n",
    "\n",
    "Gym es un API desarrollada para resolver problemas de aprendizaje por refuerzo. La intención es tener una forma práctica de representar diversos ambientes de RL (Reinforcement Learning).\n",
    "\n",
    "Las categorías principales de ambientes que están implementados en Gym son:\n",
    "* Atari\n",
    "* MuJoCo\n",
    "* Toy Text\n",
    "* Classic Control\n",
    "* Box2D\n",
    "\n",
    "Cada una de estas categorías contiene su propio conjunto de ambientes en los que agentes tienen que resolver tareas especificas. La API provee un conjunto de métodos estandarizados para comunicarse con el ambiente y poder controlarlo de manera sencilla con alguna función que establezca la política de acciones para tener éxito en resolver el reto del ambiente.\n",
    "\n",
    "En nuestro caso, 'MountainCar-v0' es la implementación de un ambiente de 'Classic Control' que puede ser manipulado fácilmente con ayuda de una política. Para ello, cada ambiente debde de ser controlado dentro de un ciclo en donde las interacciones entre el agente y el ambiente pueden ser observadas, al mismo tiempo que las observaciones y recompensas son utilizadas para que el agente tome las acciones para lograr ganar el reto.  \n",
    "\n",
    "Este ciclo básico de control en `gym` se ve de la siguiente manera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d5c4803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e5e443-7d19-43df-af24-4d4522749fd6",
   "metadata": {},
   "source": [
    "Para instalar gym:\\\n",
    "Utilizar el siguiente comando en pip.\\\n",
    "Ref:https://github.com/openai/gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4571ee2c-8c78-4bba-80f7-fe8e16e1d0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install gym[all]\n",
    "\n",
    "# En caso de solo querer instalar las dependencias de classic control utilizar:\n",
    "#!{sys.executable} -m pip install gym[classic_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cc108f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space.seed(42) =  [42]\n"
     ]
    }
   ],
   "source": [
    "env_name = \"MountainCar-v0\"\n",
    "env = gym.make(env_name, render_mode='human')\n",
    "env.action_space.seed(42)\n",
    "print('action_space.seed(42) = ', env.action_space.seed(42))\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21924db2-1450-437f-9da3-50ab635d5140",
   "metadata": {},
   "source": [
    "Vamos a analizar lo que hace este simple ejemplo.\n",
    "\n",
    "Para inicializar un ambiente hay que utilizar el método `gym.make()`. El método recibe como mínimo el argumento del nombre del ambienta que se quiere inicializar. Además, del parametro `render_mode='human'` que constantemente esta generando una imagen del estado del agente y el ambiente, generalmente utilizado para visualización humana (ver como el agente se comporta en la animación gráfica).\n",
    "\n",
    "Cada ambiente específicael formato de acciones validas a traves del atributo `env.action_space`. Y para definir el formato de observaciones validas con el atributo `env.observation_space`. Por ejemplo, para 'MountainCar-v0' tenemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd9a1946-0c9c-4e9b-9010-93e33fabf58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------MountainCar-v0 attributes-----------------------\n",
      "Observation space:  Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "Action space:\t    Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "print(f'{env_name + \" attributes\":-^70}')\n",
    "print(\"Observation space: \", env.observation_space)\n",
    "print(\"Action space:\\t   \", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8b0d14-1c46-4235-811f-7529ea51039c",
   "metadata": {},
   "source": [
    "Este ambiente esta definido por un [Space](https://www.gymlibrary.dev/api/spaces/) con un espacio de observación de tipo `Box` y un espacio de acción de tipo `Discrete` (hay distintos tipos descritos en el enlace de Space).\n",
    "* `Box`: Es un espacio en el que podemos tener los límites superior e inferior que describen los posibles valores que una observación puede tener.\n",
    "* `Discrete`: Describe un espacio discreto en el que $\\{0,1,...,n-1\\}$ son los posibles valores de las acciones u observaciones pueden tener.\n",
    "\n",
    "Antes de utilizar cualquier ambiente, hay que reiniciarlo utilizando el método `env.reset()` que regresa 'observation' y un diccionario 'info'. En nuestro caso, nos concentraremos en el valor de la observación, que será un `numpy.array` con un valor inicial dentro del `observation_space` del ambiente. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3c3823c-b796-4ad1-a5cb-a3727f0cf5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation:  [-0.4533822  0.       ]\n",
      "Observation shape:  (2,)\n"
     ]
    }
   ],
   "source": [
    "env_name = \"MountainCar-v0\"\n",
    "env = gym.make(env_name, render_mode='human')\n",
    "env.action_space.seed(42)\n",
    "observation, info = env.reset()\n",
    "print(\"Observation: \", observation)\n",
    "print(\"Observation shape: \", observation.shape)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aef7df-60f7-4909-9400-d8552958201e",
   "metadata": {},
   "source": [
    "En este caso el valor inicial de la observación contiene un `numpy.array` de forma `(2,)`. En este caso, como esta descrito en el ambiente de [MountainCar-v0](https://www.gymlibrary.dev/environments/classic_control/mountain_car/) (ver la sección 'MountainCar-v0'), este arreglo contiene la posición del agente en el eje-x y la velocidad inicial del agente. Por lo tanto, el estado inicial del agente en el ambiente sería:\n",
    "* El auto se encuentra en $-0.49841654$ en el eje x.\n",
    "* La velocidad inicial de auto es de $0$.\n",
    "\n",
    "Ahora, además de los atributos mencionados y el método de `reset()`, el [Core](https://www.gymlibrary.dev/api/core/) de la API ofrece otros métodos adicionales para poder interactuar con el agente y ambiente instanciados:\n",
    "* `step()`: Corre un paso de la dinámica del ambiente. Recibe como parámetro una acción del agente y regresa una tupla (observation, reward, terminated, truncated, info).\n",
    "* `render()`: [Ver render()](https://www.gymlibrary.dev/api/core/)\n",
    "\n",
    "Entonces, retomando el ejemplo incial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "909109ca-5d66-4248-8846-69eee58c68b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  0\n",
      "Observation:  [-0.44679132 -0.00158252]\n",
      "Reward:  -1.0\n",
      "Terminated:  False\n",
      "Truncated:  False\n",
      "Info:  {}\n",
      "Action:  2\n",
      "Observation:  [-0.4479448  -0.00115349]\n",
      "Reward:  -1.0\n",
      "Terminated:  False\n",
      "Truncated:  False\n",
      "Info:  {}\n",
      "Action:  1\n",
      "Observation:  [-0.44966084 -0.00171604]\n",
      "Reward:  -1.0\n",
      "Terminated:  False\n",
      "Truncated:  False\n",
      "Info:  {}\n",
      "Action:  1\n",
      "Observation:  [-0.4519269  -0.00226604]\n",
      "Reward:  -1.0\n",
      "Terminated:  False\n",
      "Truncated:  False\n",
      "Info:  {}\n",
      "Action:  1\n",
      "Observation:  [-0.4547263  -0.00279944]\n",
      "Reward:  -1.0\n",
      "Terminated:  False\n",
      "Truncated:  False\n",
      "Info:  {}\n",
      "Action:  2\n",
      "Observation:  [-0.45703864 -0.00231232]\n",
      "Reward:  -1.0\n",
      "Terminated:  False\n",
      "Truncated:  False\n",
      "Info:  {}\n",
      "Action:  0\n",
      "Observation:  [-0.46084684 -0.00380821]\n",
      "Reward:  -1.0\n",
      "Terminated:  False\n",
      "Truncated:  False\n",
      "Info:  {}\n",
      "Action:  2\n",
      "Observation:  [-0.46412292 -0.00327607]\n",
      "Reward:  -1.0\n",
      "Terminated:  False\n",
      "Truncated:  False\n",
      "Info:  {}\n",
      "Action:  0\n",
      "Observation:  [-0.46884272 -0.00471978]\n",
      "Reward:  -1.0\n",
      "Terminated:  False\n",
      "Truncated:  False\n",
      "Info:  {}\n",
      "Action:  0\n",
      "Observation:  [-0.47497132 -0.00612861]\n",
      "Reward:  -1.0\n",
      "Terminated:  False\n",
      "Truncated:  False\n",
      "Info:  {}\n"
     ]
    }
   ],
   "source": [
    "env_name = \"MountainCar-v0\" # Definimos nombre del ambiente\n",
    "env = gym.make(env_name, render_mode='human') # Lo instanciamos\n",
    "env.action_space.seed(42) # Para garantizar un muestreo aleatorio del espacio de acción.\n",
    "observation, info = env.reset(seed=42) # Llamamos reset()\n",
    "\n",
    "for _ in range(10): # Damos 10 pasos\n",
    "    action = env.action_space.sample() # Hacemos un muestreo del espacio de acción\n",
    "    observation, reward, terminated, truncated, info = env.step(action) # Damos un paso con la acción seleccionada\n",
    "    print(\"Action: \", action)  \n",
    "    print(\"Observation: \", observation)\n",
    "    print(\"Reward: \", reward)\n",
    "    print(\"Terminated: \", terminated)\n",
    "    print(\"Truncated: \", truncated)\n",
    "    print(\"Info: \", info)\n",
    "    if terminated or truncated: # Revisamos sí una de las dos variables que terminan el episodio es verdadera\n",
    "        observation, info = env.reset(seed=42) # Sí una de las dos es verdadera; entonces volvemos a reiniciar el ambiente\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a8a97a-f5f1-40e9-9da5-f09c2376758f",
   "metadata": {},
   "source": [
    "Para estos diez pasos cada acción depende del muestreo que hagamos del espacio de acción definido por el agente. En cada paso, vemos que la recompenza es de $-1.0$ y para la bandera de 'terminado' y 'truncado' tenemos el valor `False`. Esto debido a las condiciones para el fin del episiodio (episode end) de 'MountainCar-v0'. \n",
    "\n",
    "La intención de `gym` es brindar la posibilidad de utilizar estos métodos del API para poder controlar el agente con alguna política de función para determinar la nueva acción en cada paso. \n",
    "\n",
    "Por ejemplo:\n",
    "```\n",
    "import gym\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "   action = policy(observation)  # User-defined policy function\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "env.close()\n",
    "```\n",
    "En este caso, la variable `action` sería actualizada por una función definida que toma como parametro la observación de cada paso.\n",
    "\n",
    "Este ejemplo es muy sencillo pero la idea es dar un ejemplo de como se podría utilizar el API de Gym.\n",
    "\n",
    "## Controlando el coche con las flechas del teclado\n",
    "\n",
    "Además de poder utilizar el API para solución de problemas de RL. El API también cuenta con métodos adicionales en [Utils](https://www.gymlibrary.dev/api/utils/). como `play`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fbad955-731d-4e0f-9e49-fc1df2775d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "from gym.utils.play import play\n",
    "mapping = {(pygame.K_LEFT,): 0, (pygame.K_RIGHT,): 2, (pygame.K_DOWN,):1}\n",
    "play(gym.make(\"MountainCar-v0\",render_mode='rgb_array'), keys_to_action=mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4e00b9-f4ae-46e2-ac5f-7858d245b568",
   "metadata": {},
   "source": [
    "La utilidad `play` recibe un ambiente con un modo de renderizado y un mapeao de las teclas a utilizar para cada acción. En este caso la flecha izquierda hace que el coche acelere en esa dirección con el valor de $0$; igualmente para las demás acciones $2$ hacia la derecha y $1$ para no aceleración. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c31ee28a2e8a8c14288e32f43cdc31d04eb5922835efbdee25a698b666fb5ffc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
